The "poetry.dev-dependencies" section is deprecated and will be removed in a future version. Use "poetry.group.dev.dependencies" instead.
W0523 17:01:48.772000 1460184 site-packages/torch/distributed/run.py:766] 
W0523 17:01:48.772000 1460184 site-packages/torch/distributed/run.py:766] *****************************************
W0523 17:01:48.772000 1460184 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0523 17:01:48.772000 1460184 site-packages/torch/distributed/run.py:766] *****************************************
/baie/data1/data/expes/amrithya.balaji/scRNAseq/scbert/performer_pytorch/performer_pytorch.py:115: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2487.)
  q, r = torch.qr(unstructured_block.cpu(), some = True)
/baie/data1/data/expes/amrithya.balaji/scRNAseq/scbert/performer_pytorch/performer_pytorch.py:115: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2487.)
  q, r = torch.qr(unstructured_block.cpu(), some = True)
/baie/data1/data/expes/amrithya.balaji/scRNAseq/scbert/performer_pytorch/performer_pytorch.py:115: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2487.)
  q, r = torch.qr(unstructured_block.cpu(), some = True)
/baie/data1/data/expes/amrithya.balaji/scRNAseq/scbert/performer_pytorch/performer_pytorch.py:115: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.
The boolean parameter 'some' has been replaced with a string parameter 'mode'.
Q, R = torch.qr(A, some)
should be replaced with
Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:2487.)
  q, r = torch.qr(unstructured_block.cpu(), some = True)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/baie/data1/data/expes/amrithya.balaji/scRNAseq/scbert/besteffort_finetuning.py", line 149, in <module>
[rank0]:     model = DDP(model, device_ids=[local_rank], output_device=local_rank)
[rank0]:   File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 835, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:   File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/utils.py", line 282, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.26.2
[rank0]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank0]: Last error:
[rank0]: Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 21000
[rank3]: Traceback (most recent call last):
[rank3]:   File "/baie/data1/data/expes/amrithya.balaji/scRNAseq/scbert/besteffort_finetuning.py", line 149, in <module>
[rank3]:     model = DDP(model, device_ids=[local_rank], output_device=local_rank)
[rank3]:   File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 835, in __init__
[rank3]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank3]:   File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/utils.py", line 282, in _verify_param_shape_across_processes
[rank3]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank3]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.26.2
[rank3]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank3]: Last error:
[rank3]: Duplicate GPU detected : rank 3 and rank 2 both on CUDA device 81000
[rank2]: Traceback (most recent call last):
[rank2]:   File "/baie/data1/data/expes/amrithya.balaji/scRNAseq/scbert/besteffort_finetuning.py", line 149, in <module>
[rank2]:     model = DDP(model, device_ids=[local_rank], output_device=local_rank)
[rank2]:   File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 835, in __init__
[rank2]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank2]:   File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/utils.py", line 282, in _verify_param_shape_across_processes
[rank2]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank2]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.26.2
[rank2]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank2]: Last error:
[rank2]: Duplicate GPU detected : rank 2 and rank 3 both on CUDA device 81000
[rank1]: Traceback (most recent call last):
[rank1]:   File "/baie/data1/data/expes/amrithya.balaji/scRNAseq/scbert/besteffort_finetuning.py", line 149, in <module>
[rank1]:     model = DDP(model, device_ids=[local_rank], output_device=local_rank)
[rank1]:   File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 835, in __init__
[rank1]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank1]:   File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/utils.py", line 282, in _verify_param_shape_across_processes
[rank1]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank1]: torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.26.2
[rank1]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank1]: Last error:
[rank1]: Duplicate GPU detected : rank 1 and rank 0 both on CUDA device 21000
[rank0]:[W523 17:02:08.742042439 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0523 17:02:09.729000 1460184 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1460231 closing signal SIGTERM
W0523 17:02:09.730000 1460184 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1460232 closing signal SIGTERM
W0523 17:02:09.730000 1460184 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1460233 closing signal SIGTERM
E0523 17:02:10.021000 1460184 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1460230) of binary: /public/conda/user_envs/amrithya.balaji/envs/arb/bin/python
Traceback (most recent call last):
  File "/public/conda/user_envs/amrithya.balaji/envs/arb/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in main
    run(args)
  File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/public/conda/user_envs/amrithya.balaji/envs/arb/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
besteffort_finetuning.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-23_17:02:09
  host      : mundus-mir-2.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1460230)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
